{
 "cells": [
  {
   "cell_type": "code",
   "id": "35af0992",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T12:30:40.683896Z",
     "start_time": "2025-08-02T12:30:34.836399Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from cotracker.utils.visualizer import Visualizer, read_video_from_path\n",
    "from cotracker.models.core.model_utils import get_points_on_a_grid\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.ndimage import map_coordinates\n",
    "import math\n",
    "\n",
    "#tried to clear memory because of OOM errors\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "DEFAULT_DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2bd32ab4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T12:30:40.893716Z",
     "start_time": "2025-08-02T12:30:40.889726Z"
    }
   },
   "source": [
    "def load_video(file, size=(256,256)):\n",
    "    \"\"\"\n",
    "    Load a video from the given file path, resize it to the specified size,\n",
    "    and return the video tensor and frames. \n",
    "    args:\n",
    "        file (str): Path to the video file.\n",
    "        size (tuple): Desired size for the video frames (width, height).\n",
    "    returns:\n",
    "        video (torch.Tensor): A tensor containing the video frames, resized to the specified size.\n",
    "        video_frames (list): A list of resized video frames as numpy arrays.\n",
    "    \"\"\"\n",
    "    video_path = file\n",
    "    video = read_video_from_path(video_path)\n",
    "    video_frames = [cv2.resize(frame, size) for frame in video]\n",
    "    video = torch.from_numpy(video).permute(0, 3, 1, 2).float()\n",
    "    \n",
    "    video = F.interpolate(video, size=size, mode='bilinear', align_corners=False)[None]\n",
    "\n",
    "    return video, video_frames"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "078475cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T12:30:41.078403Z",
     "start_time": "2025-08-02T12:30:41.072402Z"
    }
   },
   "source": [
    "def cotrack_model(video, grid_size):\n",
    "    \"\"\"\"Run the CoTracker model on the provided video and grid points.\n",
    "    Model defines which cotracker model is used for tracking.\n",
    "    args:\n",
    "        video (torch.Tensor): A tensor containing the video frames.\n",
    "        grid_size (int): The size of the grid for tracking.\n",
    "        points_x (list): List of x-coordinates for reference points.\n",
    "        points_y (list): List of y-coordinates for reference points.\n",
    "    returns:\n",
    "        pred_tracks (torch.Tensor): Predicted tracks from the model.\n",
    "        pred_visibility (torch.Tensor): Predicted visibility from the model.\n",
    "        grid_pts (torch.Tensor): Points on the grid used for tracking.\n",
    "    \"\"\"\n",
    "    model = torch.hub.load(\"facebookresearch/co-tracker\", \"cotracker3_offline\").to(\"cuda\")\n",
    "    model = model.to(DEFAULT_DEVICE)\n",
    "    video = video.to(DEFAULT_DEVICE)\n",
    "    model.model.model_resolution = video.shape[3:]\n",
    "    grid_pts = get_points_on_a_grid(\n",
    "                grid_size, model.model.model_resolution\n",
    "            )\n",
    "\n",
    "    pred_tracks, pred_visibility = model(\n",
    "        video,\n",
    "        grid_size=grid_size,\n",
    "        grid_query_frame=0,\n",
    "        backward_tracking=True,\n",
    "    )\n",
    "    return pred_tracks, pred_visibility, grid_pts"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "5039bd8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T12:30:41.131772Z",
     "start_time": "2025-08-02T12:30:41.124929Z"
    }
   },
   "source": [
    "def warping(predicted_tracks, frames):      \n",
    "    \"\"\"\n",
    "    Warps the frames of a video based on predicted tracks.\n",
    "    args:       \n",
    "        predicted_tracks (torch.Tensor): Predicted tracks of shape (B, T, G, D).\n",
    "        frames (torch.Tensor): Video frames of shape (T_orig, H, W, C).\n",
    "    returns:\n",
    "        warpeds (list): List of warped frames.\n",
    "    \"\"\"\n",
    "    T_orig, H, W, C = frames.shape\n",
    "    B, T, G, D = predicted_tracks.shape\n",
    "    grid_size = int(math.sqrt(G))\n",
    "\n",
    "    velocity = predicted_tracks[0].reshape(T, grid_size, grid_size, 2) #(24, 32, 32, 2)\n",
    "    real_velocity = velocity-velocity[0] # (24, 32, 32, 2)\n",
    "    v = real_velocity.transpose(0, 3, 1, 2) # (24, 2, 32, 32)\n",
    "    vp = zoom(v, (1, 1, W/grid_size, H/grid_size))  #(24, 2, 256, 256) \n",
    "        \n",
    "    warpeds = [frames[0][...,0]]\n",
    "\n",
    "    for i in range(1,T_orig):\n",
    "        grid_x, grid_y = np.meshgrid(np.arange(W), np.arange(H))\n",
    "        grid_x = grid_x.astype(np.float32)\n",
    "        grid_y = grid_y.astype(np.float32)\n",
    "\n",
    "        phi = np.diff(vp,axis=0)[0:i].sum(0)\n",
    "        grid_x += phi[0]\n",
    "        grid_y += phi[1]\n",
    "  \n",
    "        warped = map_coordinates(frames[i][...,0].astype(np.float32), [grid_y, grid_x], order=3, mode='nearest')\n",
    "        warpeds.append(warped)\n",
    "       \n",
    "    return np.array(warpeds)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "7df2848e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T14:41:43.436939Z",
     "start_time": "2025-08-02T14:41:41.210128Z"
    }
   },
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "#define the video file path\n",
    "video_file = \"../../data/input/low_movement/1czi.tif\"\n",
    "if not os.path.exists(video_file):\n",
    "    print(f\"Video file {video_file} does not exist.\")\n",
    "else:\n",
    "    print(f\"Video file {video_file} exists.\")\n",
    "\n",
    "# Apply motion correction algorithm by\n",
    "# loading the video, running the CoTracker model, and warping the frames\n",
    "\n",
    "vid = load_video(video_file, size=(256, 256))\n",
    "pred_tracks = cotrack_model(vid[0], grid_size=16)\n",
    "result = warping(pred_tracks[0].cpu().numpy(), np.array(vid[1]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video file ../../data/input/low_movement/1czi.tif exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\morit/.cache\\torch\\hub\\facebookresearch_co-tracker_main\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1, 3, 384, 512]' is invalid for input of size 100663296",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# Apply motion correction algorithm by\u001B[39;00m\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# loading the video, running the CoTracker model, and warping the frames\u001B[39;00m\n\u001B[32m     14\u001B[39m vid = load_video(video_file, size=(\u001B[32m256\u001B[39m, \u001B[32m256\u001B[39m))\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m pred_tracks = \u001B[43mcotrack_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvid\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m16\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m result = warping(pred_tracks[\u001B[32m0\u001B[39m].cpu().numpy(), np.array(vid[\u001B[32m1\u001B[39m]))\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 22\u001B[39m, in \u001B[36mcotrack_model\u001B[39m\u001B[34m(video, grid_size)\u001B[39m\n\u001B[32m     17\u001B[39m model.model.model_resolution = video.shape[\u001B[32m3\u001B[39m:]\n\u001B[32m     18\u001B[39m grid_pts = get_points_on_a_grid(\n\u001B[32m     19\u001B[39m             grid_size, model.model.model_resolution\n\u001B[32m     20\u001B[39m         )\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m pred_tracks, pred_visibility = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvideo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrid_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgrid_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrid_query_frame\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbackward_tracking\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m pred_tracks, pred_visibility, grid_pts\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Studium\\BIMAP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Studium\\BIMAP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Studium\\BIMAP\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Studium\\BIMAP\\cotracker\\predictor.py:58\u001B[39m, in \u001B[36mCoTrackerPredictor.forward\u001B[39m\u001B[34m(self, video, queries, segm_mask, grid_size, grid_query_frame, backward_tracking)\u001B[39m\n\u001B[32m     52\u001B[39m     tracks, visibilities = \u001B[38;5;28mself\u001B[39m._compute_dense_tracks(\n\u001B[32m     53\u001B[39m         video,\n\u001B[32m     54\u001B[39m         grid_query_frame=grid_query_frame,\n\u001B[32m     55\u001B[39m         backward_tracking=backward_tracking,\n\u001B[32m     56\u001B[39m     )\n\u001B[32m     57\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m58\u001B[39m     tracks, visibilities = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_compute_sparse_tracks\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     59\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvideo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[43m        \u001B[49m\u001B[43mqueries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     61\u001B[39m \u001B[43m        \u001B[49m\u001B[43msegm_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     62\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrid_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     63\u001B[39m \u001B[43m        \u001B[49m\u001B[43madd_support_grid\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrid_size\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msegm_mask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrid_query_frame\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgrid_query_frame\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbackward_tracking\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbackward_tracking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     66\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m tracks, visibilities\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Studium\\BIMAP\\cotracker\\predictor.py:116\u001B[39m, in \u001B[36mCoTrackerPredictor._compute_sparse_tracks\u001B[39m\u001B[34m(self, video, queries, segm_mask, grid_size, add_support_grid, grid_query_frame, backward_tracking)\u001B[39m\n\u001B[32m    112\u001B[39m video = video.reshape(B * T, C, H, W)\n\u001B[32m    113\u001B[39m video = F.interpolate(\n\u001B[32m    114\u001B[39m     video, \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28mself\u001B[39m.interp_shape), mode=\u001B[33m\"\u001B[39m\u001B[33mbilinear\u001B[39m\u001B[33m\"\u001B[39m, align_corners=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    115\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m video = \u001B[43mvideo\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mB\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minterp_shape\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minterp_shape\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m queries \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    119\u001B[39m     B, N, D = queries.shape\n",
      "\u001B[31mRuntimeError\u001B[39m: shape '[1, 1, 3, 384, 512]' is invalid for input of size 100663296"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b85cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out warped results\n",
    "for i in range(result.shape[0]):\n",
    "    cv2.imwrite(f\"output/frame_{i:04d}.png\", result[i])  # Save each frame as an image\n",
    "print(\"Warped frames saved to output directory.\")"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T12:30:41.521403600Z",
     "start_time": "2025-07-04T16:13:18.266435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "nx, ny = (3, 2)\n",
    "\n",
    "x = np.linspace(0, 1, nx)\n",
    "print(x.shape)\n",
    "\n",
    "y = np.linspace(0, 1, ny)\n",
    "print(y.shape)\n",
    "\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "xv.shape"
   ],
   "id": "f719c51f377d6d71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(2,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "599e7cf3ef3b940"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracker-jn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
